{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_XWQDVZ3KP76"
      },
      "outputs": [],
      "source": [
        "#Load raw text we want to work with\n",
        "#The Verdict by Edith Wharton is a public domain short story\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/LLMs-from-scratch/ch02/01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A28bLDu9MvME",
        "outputId": "54a66d77-9d75-4562-8988-c951ab2ce768"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The following regular expression will split on whitespaces\n",
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD3bp86gN_4b",
        "outputId": "f0aced8f-313a-4b2a-b046-89557c9b5277"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well\n",
        "\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqejpCSbOhPK",
        "outputId": "c1f2fe8a-6ea7-4801-8dd3-dbdeb1d49699"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's also handle other types of punctuation, such as periods, question marks, and so on\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NExM7RziOzQd",
        "outputId": "e63c0fef-b637-4ccb-febb-937745384e51"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKk4N5iRPFcD",
        "outputId": "fe4df16d-e81f-4bb6-f6e4-0e8fb6cc52e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's calculate the total number of tokens\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPo9PF5NPNJ6",
        "outputId": "269d8d7c-f491-4111-9b8e-883c89bc0bbf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#From these tokens, we can now build a vocabulary that consists of all the unique tokens\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyPGFgpPPRjK",
        "outputId": "161cef01-5a76-445c-dfec-b388da1f6bc2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "dwg5A8IZPkkK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Below are the first 50 entries in this vocabulary:\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 850:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_JJ5DpbPr2y",
        "outputId": "fba921c5-94a9-42dc-abfd-9d1ef59818e3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Carlo;', 25)\n",
            "('Chicago', 26)\n",
            "('Claude', 27)\n",
            "('Come', 28)\n",
            "('Croft', 29)\n",
            "('Destroyed', 30)\n",
            "('Devonshire', 31)\n",
            "('Don', 32)\n",
            "('Dubarry', 33)\n",
            "('Emperors', 34)\n",
            "('Florence', 35)\n",
            "('For', 36)\n",
            "('Gallery', 37)\n",
            "('Gideon', 38)\n",
            "('Gisburn', 39)\n",
            "('Gisburns', 40)\n",
            "('Grafton', 41)\n",
            "('Greek', 42)\n",
            "('Grindle', 43)\n",
            "('Grindle:', 44)\n",
            "('Grindles', 45)\n",
            "('HAD', 46)\n",
            "('Had', 47)\n",
            "('Hang', 48)\n",
            "('Has', 49)\n",
            "('He', 50)\n",
            "('Her', 51)\n",
            "('Hermia', 52)\n",
            "('His', 53)\n",
            "('How', 54)\n",
            "('I', 55)\n",
            "('If', 56)\n",
            "('In', 57)\n",
            "('It', 58)\n",
            "('Jack', 59)\n",
            "('Jove', 60)\n",
            "('Just', 61)\n",
            "('Lord', 62)\n",
            "('Made', 63)\n",
            "('Miss', 64)\n",
            "('Money', 65)\n",
            "('Monte', 66)\n",
            "('Moon-dancers', 67)\n",
            "('Mr', 68)\n",
            "('Mrs', 69)\n",
            "('My', 70)\n",
            "('Never', 71)\n",
            "('No', 72)\n",
            "('Now', 73)\n",
            "('Nutley', 74)\n",
            "('Of', 75)\n",
            "('Oh', 76)\n",
            "('On', 77)\n",
            "('Once', 78)\n",
            "('Only', 79)\n",
            "('Or', 80)\n",
            "('Perhaps', 81)\n",
            "('Poor', 82)\n",
            "('Professional', 83)\n",
            "('Renaissance', 84)\n",
            "('Rickham', 85)\n",
            "('Rickham;', 86)\n",
            "('Riviera', 87)\n",
            "('Rome', 88)\n",
            "('Russian', 89)\n",
            "('Sevres', 90)\n",
            "('She', 91)\n",
            "('Stroud', 92)\n",
            "('Strouds', 93)\n",
            "('Suddenly', 94)\n",
            "('That', 95)\n",
            "('The', 96)\n",
            "('Then', 97)\n",
            "('There', 98)\n",
            "('There:', 99)\n",
            "('They', 100)\n",
            "('This', 101)\n",
            "('Those', 102)\n",
            "('Though', 103)\n",
            "('Thwing', 104)\n",
            "('Thwings', 105)\n",
            "('To', 106)\n",
            "('Usually', 107)\n",
            "('Venetian', 108)\n",
            "('Victor', 109)\n",
            "('Was', 110)\n",
            "('We', 111)\n",
            "('Well', 112)\n",
            "('What', 113)\n",
            "('When', 114)\n",
            "('Why', 115)\n",
            "('Yes', 116)\n",
            "('You', 117)\n",
            "('_', 118)\n",
            "('a', 119)\n",
            "('abdication', 120)\n",
            "('able', 121)\n",
            "('about', 122)\n",
            "('about;', 123)\n",
            "('above', 124)\n",
            "('abruptly', 125)\n",
            "('absolute', 126)\n",
            "('absorbed', 127)\n",
            "('absurdity', 128)\n",
            "('academic', 129)\n",
            "('accuse', 130)\n",
            "('accustomed', 131)\n",
            "('across', 132)\n",
            "('activity', 133)\n",
            "('add', 134)\n",
            "('added', 135)\n",
            "('admirers', 136)\n",
            "('adopted', 137)\n",
            "('adulation', 138)\n",
            "('advance', 139)\n",
            "('aesthetic', 140)\n",
            "('affect', 141)\n",
            "('afraid', 142)\n",
            "('after', 143)\n",
            "('afterward', 144)\n",
            "('again', 145)\n",
            "('ago', 146)\n",
            "('ah', 147)\n",
            "('air', 148)\n",
            "('alive', 149)\n",
            "('all', 150)\n",
            "('almost', 151)\n",
            "('alone', 152)\n",
            "('along', 153)\n",
            "('always', 154)\n",
            "('am', 155)\n",
            "('amazement', 156)\n",
            "('amid', 157)\n",
            "('among', 158)\n",
            "('amplest', 159)\n",
            "('amusing', 160)\n",
            "('an', 161)\n",
            "('and', 162)\n",
            "('another', 163)\n",
            "('answer', 164)\n",
            "('answered', 165)\n",
            "('any', 166)\n",
            "('anything', 167)\n",
            "('anywhere', 168)\n",
            "('apparent', 169)\n",
            "('apparently', 170)\n",
            "('appearance', 171)\n",
            "('appeared', 172)\n",
            "('appointed', 173)\n",
            "('are', 174)\n",
            "('arm', 175)\n",
            "('arm-chair', 176)\n",
            "('arm-chairs', 177)\n",
            "('arms', 178)\n",
            "('art', 179)\n",
            "('articles', 180)\n",
            "('artist', 181)\n",
            "('as', 182)\n",
            "('aside', 183)\n",
            "('asked', 184)\n",
            "('at', 185)\n",
            "('atmosphere', 186)\n",
            "('atom', 187)\n",
            "('attack', 188)\n",
            "('attention', 189)\n",
            "('attention;', 190)\n",
            "('attitude', 191)\n",
            "('audacities', 192)\n",
            "('away', 193)\n",
            "('awful', 194)\n",
            "('axioms', 195)\n",
            "('azaleas', 196)\n",
            "('back', 197)\n",
            "('background', 198)\n",
            "('balance', 199)\n",
            "('balancing', 200)\n",
            "('balustraded', 201)\n",
            "('basking', 202)\n",
            "('bath-rooms', 203)\n",
            "('be', 204)\n",
            "('beaming', 205)\n",
            "('bean-stalk', 206)\n",
            "('bear', 207)\n",
            "('beard', 208)\n",
            "('beauty', 209)\n",
            "('became', 210)\n",
            "('because', 211)\n",
            "('becoming', 212)\n",
            "('bed', 213)\n",
            "('been', 214)\n",
            "('before', 215)\n",
            "('began', 216)\n",
            "('begun', 217)\n",
            "('behind', 218)\n",
            "('being', 219)\n",
            "('believed', 220)\n",
            "('beneath', 221)\n",
            "('bespoke', 222)\n",
            "('better', 223)\n",
            "('better;', 224)\n",
            "('between', 225)\n",
            "('big', 226)\n",
            "('bits', 227)\n",
            "('bitterness', 228)\n",
            "('blocked', 229)\n",
            "('born', 230)\n",
            "('borne', 231)\n",
            "('boudoir', 232)\n",
            "('bravura', 233)\n",
            "('break', 234)\n",
            "('breaking', 235)\n",
            "('breathing', 236)\n",
            "('bric-a-brac', 237)\n",
            "('briefly', 238)\n",
            "('brings', 239)\n",
            "('bronzes', 240)\n",
            "('brought', 241)\n",
            "('brown', 242)\n",
            "('brush', 243)\n",
            "('bull', 244)\n",
            "('business', 245)\n",
            "('but', 246)\n",
            "('buying', 247)\n",
            "('by', 248)\n",
            "('called', 249)\n",
            "('came', 250)\n",
            "('can', 251)\n",
            "('canvas', 252)\n",
            "('canvases', 253)\n",
            "('cards', 254)\n",
            "('care', 255)\n",
            "('career', 256)\n",
            "('caught', 257)\n",
            "('central', 258)\n",
            "('chair', 259)\n",
            "('chap', 260)\n",
            "('characteristic', 261)\n",
            "('charming', 262)\n",
            "('cheap', 263)\n",
            "('check', 264)\n",
            "('cheeks', 265)\n",
            "('chest', 266)\n",
            "('chimney-piece', 267)\n",
            "('chucked', 268)\n",
            "('cigar', 269)\n",
            "('cigarette', 270)\n",
            "('cigars', 271)\n",
            "('circulation', 272)\n",
            "('circumstance', 273)\n",
            "('circus-clown', 274)\n",
            "('claimed', 275)\n",
            "('clasping', 276)\n",
            "('clear', 277)\n",
            "('cleverer', 278)\n",
            "('close', 279)\n",
            "('clue', 280)\n",
            "('coat', 281)\n",
            "('collapsed', 282)\n",
            "('colour', 283)\n",
            "('come', 284)\n",
            "('comfortable', 285)\n",
            "('coming', 286)\n",
            "('companion', 287)\n",
            "('compared', 288)\n",
            "('complex', 289)\n",
            "('confident', 290)\n",
            "('congesting', 291)\n",
            "('conjugal', 292)\n",
            "('constraint', 293)\n",
            "('consummate', 294)\n",
            "('contended', 295)\n",
            "('continued', 296)\n",
            "('corner', 297)\n",
            "('corrected', 298)\n",
            "('could', 299)\n",
            "('couldn', 300)\n",
            "('count', 301)\n",
            "('countenance', 302)\n",
            "('couple', 303)\n",
            "('course', 304)\n",
            "('covered', 305)\n",
            "('craft', 306)\n",
            "('cried', 307)\n",
            "('crossed', 308)\n",
            "('crowned', 309)\n",
            "('crumbled', 310)\n",
            "('cry', 311)\n",
            "('cured', 312)\n",
            "('curiosity', 313)\n",
            "('curious', 314)\n",
            "('current', 315)\n",
            "('curtains', 316)\n",
            "('d', 317)\n",
            "('dabble', 318)\n",
            "('damask', 319)\n",
            "('dark', 320)\n",
            "('dashed', 321)\n",
            "('day', 322)\n",
            "('days', 323)\n",
            "('dead', 324)\n",
            "('deadening', 325)\n",
            "('dear', 326)\n",
            "('deep', 327)\n",
            "('deerhound', 328)\n",
            "('degree', 329)\n",
            "('delicate', 330)\n",
            "('demand', 331)\n",
            "('denied', 332)\n",
            "('deploring', 333)\n",
            "('deprecating', 334)\n",
            "('deprecatingly', 335)\n",
            "('desire', 336)\n",
            "('destroyed', 337)\n",
            "('destruction', 338)\n",
            "('desultory', 339)\n",
            "('detail', 340)\n",
            "('diagnosis', 341)\n",
            "('did', 342)\n",
            "('didn', 343)\n",
            "('died', 344)\n",
            "('dim', 345)\n",
            "('dimmest', 346)\n",
            "('dingy', 347)\n",
            "('dining-room', 348)\n",
            "('disarming', 349)\n",
            "('discovery;', 350)\n",
            "('discrimination', 351)\n",
            "('discussion', 352)\n",
            "('disdain', 353)\n",
            "('disdained', 354)\n",
            "('disease', 355)\n",
            "('disguised', 356)\n",
            "('display', 357)\n",
            "('dissatisfied', 358)\n",
            "('distinguished', 359)\n",
            "('distract', 360)\n",
            "('divert', 361)\n",
            "('do', 362)\n",
            "('doesn', 363)\n",
            "('doing', 364)\n",
            "('domestic', 365)\n",
            "('don', 366)\n",
            "('done', 367)\n",
            "('donkey', 368)\n",
            "('down', 369)\n",
            "('dozen', 370)\n",
            "('dragged', 371)\n",
            "('drawing-room', 372)\n",
            "('drawing-rooms', 373)\n",
            "('drawn', 374)\n",
            "('dress-closets', 375)\n",
            "('drew', 376)\n",
            "('dropped', 377)\n",
            "('each', 378)\n",
            "('earth', 379)\n",
            "('ease', 380)\n",
            "('easel', 381)\n",
            "('easy', 382)\n",
            "('echoed', 383)\n",
            "('economy', 384)\n",
            "('effect', 385)\n",
            "('effects', 386)\n",
            "('efforts', 387)\n",
            "('egregious', 388)\n",
            "('eighteenth-century', 389)\n",
            "('elbow', 390)\n",
            "('elegant', 391)\n",
            "('else', 392)\n",
            "('embarrassed', 393)\n",
            "('enabled', 394)\n",
            "('end', 395)\n",
            "('endless', 396)\n",
            "('enjoy', 397)\n",
            "('enlightenment:', 398)\n",
            "('enough', 399)\n",
            "('ensuing', 400)\n",
            "('equally', 401)\n",
            "('equanimity', 402)\n",
            "('escape', 403)\n",
            "('established', 404)\n",
            "('etching', 405)\n",
            "('even', 406)\n",
            "('event', 407)\n",
            "('ever', 408)\n",
            "('everlasting', 409)\n",
            "('every', 410)\n",
            "('exasperated', 411)\n",
            "('except', 412)\n",
            "('excuse', 413)\n",
            "('excusing', 414)\n",
            "('existed', 415)\n",
            "('expected', 416)\n",
            "('exquisite', 417)\n",
            "('exquisitely', 418)\n",
            "('extenuation', 419)\n",
            "('exterminating', 420)\n",
            "('extracting', 421)\n",
            "('eye', 422)\n",
            "('eyebrows', 423)\n",
            "('eyes', 424)\n",
            "('eyes:', 425)\n",
            "('face', 426)\n",
            "('faces', 427)\n",
            "('fact', 428)\n",
            "('faded', 429)\n",
            "('failed', 430)\n",
            "('failure', 431)\n",
            "('fair', 432)\n",
            "('faith', 433)\n",
            "('false', 434)\n",
            "('familiar', 435)\n",
            "('famille-verte', 436)\n",
            "('fancy', 437)\n",
            "('fashionable', 438)\n",
            "('fate', 439)\n",
            "('feather', 440)\n",
            "('feet', 441)\n",
            "('fell', 442)\n",
            "('fellow', 443)\n",
            "('felt', 444)\n",
            "('few', 445)\n",
            "('fewer', 446)\n",
            "('finality', 447)\n",
            "('find', 448)\n",
            "('fingers', 449)\n",
            "('first', 450)\n",
            "('fit', 451)\n",
            "('fitting', 452)\n",
            "('five', 453)\n",
            "('flash', 454)\n",
            "('flashed', 455)\n",
            "('florid', 456)\n",
            "('flowers', 457)\n",
            "('fluently', 458)\n",
            "('flung', 459)\n",
            "('follow', 460)\n",
            "('followed', 461)\n",
            "('fond', 462)\n",
            "('footstep', 463)\n",
            "('for', 464)\n",
            "('forced', 465)\n",
            "('forcing', 466)\n",
            "('forehead', 467)\n",
            "('foreign', 468)\n",
            "('foreseen', 469)\n",
            "('forgive', 470)\n",
            "('forgotten', 471)\n",
            "('form', 472)\n",
            "('formed', 473)\n",
            "('forming', 474)\n",
            "('forward', 475)\n",
            "('fostered', 476)\n",
            "('found', 477)\n",
            "('foundations', 478)\n",
            "('fragment', 479)\n",
            "('fragments', 480)\n",
            "('frame', 481)\n",
            "('frames', 482)\n",
            "('frequently', 483)\n",
            "('friend', 484)\n",
            "('from', 485)\n",
            "('full', 486)\n",
            "('fullest', 487)\n",
            "('furiously', 488)\n",
            "('furrowed', 489)\n",
            "('garlanded', 490)\n",
            "('garlands', 491)\n",
            "('gave', 492)\n",
            "('genial', 493)\n",
            "('genius', 494)\n",
            "('gesture', 495)\n",
            "('get', 496)\n",
            "('getting', 497)\n",
            "('give', 498)\n",
            "('given', 499)\n",
            "('glad', 500)\n",
            "('glanced', 501)\n",
            "('glimpse', 502)\n",
            "('gloried', 503)\n",
            "('glory', 504)\n",
            "('go', 505)\n",
            "('going', 506)\n",
            "('gone', 507)\n",
            "('good', 508)\n",
            "('good-breeding', 509)\n",
            "('good-humoured', 510)\n",
            "('got', 511)\n",
            "('grace', 512)\n",
            "('gradually', 513)\n",
            "('gray', 514)\n",
            "('grayish', 515)\n",
            "('great', 516)\n",
            "('greatest', 517)\n",
            "('greatness', 518)\n",
            "('grew', 519)\n",
            "('groping', 520)\n",
            "('growing', 521)\n",
            "('had', 522)\n",
            "('hadn', 523)\n",
            "('hair', 524)\n",
            "('half', 525)\n",
            "('half-light', 526)\n",
            "('half-mechanically', 527)\n",
            "('hall', 528)\n",
            "('hand', 529)\n",
            "('hands', 530)\n",
            "('handsome', 531)\n",
            "('hanging', 532)\n",
            "('happen', 533)\n",
            "('happened', 534)\n",
            "('hard', 535)\n",
            "('hardly', 536)\n",
            "('has', 537)\n",
            "('have', 538)\n",
            "('haven', 539)\n",
            "('having', 540)\n",
            "('he', 541)\n",
            "('head', 542)\n",
            "('hear', 543)\n",
            "('heard', 544)\n",
            "('heart', 545)\n",
            "('height', 546)\n",
            "('her', 547)\n",
            "('here', 548)\n",
            "('here;', 549)\n",
            "('hermit', 550)\n",
            "('herself', 551)\n",
            "('hesitations', 552)\n",
            "('hide', 553)\n",
            "('high', 554)\n",
            "('him', 555)\n",
            "('him:', 556)\n",
            "('himself', 557)\n",
            "('hint', 558)\n",
            "('his', 559)\n",
            "('history', 560)\n",
            "('holding', 561)\n",
            "('home', 562)\n",
            "('honour', 563)\n",
            "('hooded', 564)\n",
            "('hostess', 565)\n",
            "('hostess:', 566)\n",
            "('hot-house', 567)\n",
            "('hour', 568)\n",
            "('hours', 569)\n",
            "('house', 570)\n",
            "('how', 571)\n",
            "('hung', 572)\n",
            "('husband', 573)\n",
            "('idea', 574)\n",
            "('idle', 575)\n",
            "('idling', 576)\n",
            "('if', 577)\n",
            "('immediately', 578)\n",
            "('in', 579)\n",
            "('incense', 580)\n",
            "('indifferent;', 581)\n",
            "('inevitable', 582)\n",
            "('inevitably', 583)\n",
            "('inflexible', 584)\n",
            "('insensible', 585)\n",
            "('insignificant', 586)\n",
            "('instinctively', 587)\n",
            "('instructive', 588)\n",
            "('interesting', 589)\n",
            "('into', 590)\n",
            "('ironic', 591)\n",
            "('irony', 592)\n",
            "('irrelevance', 593)\n",
            "('irrevocable', 594)\n",
            "('is', 595)\n",
            "('it', 596)\n",
            "('it;', 597)\n",
            "('its', 598)\n",
            "('itself', 599)\n",
            "('jardiniere', 600)\n",
            "('jealousy', 601)\n",
            "('just', 602)\n",
            "('keep', 603)\n",
            "('kept', 604)\n",
            "('kind', 605)\n",
            "('knees', 606)\n",
            "('knew', 607)\n",
            "('know', 608)\n",
            "('know;', 609)\n",
            "('known', 610)\n",
            "('laid', 611)\n",
            "('lair', 612)\n",
            "('landing', 613)\n",
            "('language', 614)\n",
            "('last', 615)\n",
            "('late', 616)\n",
            "('later', 617)\n",
            "('latter', 618)\n",
            "('laugh', 619)\n",
            "('laugh:', 620)\n",
            "('laughed', 621)\n",
            "('lay', 622)\n",
            "('leading', 623)\n",
            "('lean', 624)\n",
            "('learned', 625)\n",
            "('least', 626)\n",
            "('leathery:', 627)\n",
            "('leave', 628)\n",
            "('led', 629)\n",
            "('left', 630)\n",
            "('leisure', 631)\n",
            "('lends', 632)\n",
            "('lent', 633)\n",
            "('let', 634)\n",
            "('lies', 635)\n",
            "('life', 636)\n",
            "('life-likeness', 637)\n",
            "('lift', 638)\n",
            "('lifted', 639)\n",
            "('light', 640)\n",
            "('lightly;', 641)\n",
            "('like', 642)\n",
            "('liked', 643)\n",
            "('line', 644)\n",
            "('lines', 645)\n",
            "('lingered', 646)\n",
            "('lips', 647)\n",
            "('lit', 648)\n",
            "('little', 649)\n",
            "('little:', 650)\n",
            "('live', 651)\n",
            "('ll', 652)\n",
            "('loathing', 653)\n",
            "('long', 654)\n",
            "('longed', 655)\n",
            "('longer', 656)\n",
            "('look', 657)\n",
            "('looked', 658)\n",
            "('looking', 659)\n",
            "('lose', 660)\n",
            "('loss', 661)\n",
            "('lounging', 662)\n",
            "('lovely', 663)\n",
            "('lucky', 664)\n",
            "('lump', 665)\n",
            "('luncheon-table', 666)\n",
            "('luxury', 667)\n",
            "('lying', 668)\n",
            "('made', 669)\n",
            "('make', 670)\n",
            "('man', 671)\n",
            "('manage', 672)\n",
            "('managed', 673)\n",
            "('mantel-piece', 674)\n",
            "('marble', 675)\n",
            "('married', 676)\n",
            "('may', 677)\n",
            "('me', 678)\n",
            "('meant', 679)\n",
            "('mediocrity', 680)\n",
            "('medium', 681)\n",
            "('mentioned', 682)\n",
            "('mere', 683)\n",
            "('merely', 684)\n",
            "('met', 685)\n",
            "('might', 686)\n",
            "('mighty', 687)\n",
            "('millionaire', 688)\n",
            "('mine', 689)\n",
            "('mine:', 690)\n",
            "('minute', 691)\n",
            "('minutes', 692)\n",
            "('mirrors', 693)\n",
            "('modest', 694)\n",
            "('modesty', 695)\n",
            "('moment', 696)\n",
            "('money', 697)\n",
            "('monumental', 698)\n",
            "('mood', 699)\n",
            "('morbidly', 700)\n",
            "('more', 701)\n",
            "('most', 702)\n",
            "('mourn', 703)\n",
            "('mourned', 704)\n",
            "('moustache', 705)\n",
            "('moved', 706)\n",
            "('much', 707)\n",
            "('muddling;', 708)\n",
            "('multiplied', 709)\n",
            "('murmur', 710)\n",
            "('muscles', 711)\n",
            "('must', 712)\n",
            "('my', 713)\n",
            "('myself', 714)\n",
            "('mysterious', 715)\n",
            "('naive', 716)\n",
            "('near', 717)\n",
            "('nearly', 718)\n",
            "('negatived', 719)\n",
            "('nervous', 720)\n",
            "('nervousness;', 721)\n",
            "('neutral', 722)\n",
            "('never', 723)\n",
            "('next', 724)\n",
            "('no', 725)\n",
            "('none', 726)\n",
            "('not', 727)\n",
            "('note', 728)\n",
            "('nothing', 729)\n",
            "('now', 730)\n",
            "('nymphs', 731)\n",
            "('oak', 732)\n",
            "('obituary', 733)\n",
            "('object', 734)\n",
            "('objects', 735)\n",
            "('occurred', 736)\n",
            "('oddly', 737)\n",
            "('of', 738)\n",
            "('off', 739)\n",
            "('often', 740)\n",
            "('oh', 741)\n",
            "('old', 742)\n",
            "('on', 743)\n",
            "('once', 744)\n",
            "('one', 745)\n",
            "('ones', 746)\n",
            "('only', 747)\n",
            "('onto', 748)\n",
            "('open', 749)\n",
            "('or', 750)\n",
            "('other', 751)\n",
            "('our', 752)\n",
            "('ourselves', 753)\n",
            "('out', 754)\n",
            "('out:', 755)\n",
            "('outline', 756)\n",
            "('oval', 757)\n",
            "('over', 758)\n",
            "('own', 759)\n",
            "('packed', 760)\n",
            "('paid', 761)\n",
            "('paint', 762)\n",
            "('painted', 763)\n",
            "('painted;', 764)\n",
            "('painter', 765)\n",
            "('painting', 766)\n",
            "('painting;', 767)\n",
            "('pale', 768)\n",
            "('paled', 769)\n",
            "('palm-trees;', 770)\n",
            "('panel', 771)\n",
            "('panelling', 772)\n",
            "('pardonable', 773)\n",
            "('pardoned', 774)\n",
            "('part', 775)\n",
            "('passages', 776)\n",
            "('passing', 777)\n",
            "('past', 778)\n",
            "('pastels', 779)\n",
            "('pathos', 780)\n",
            "('patient', 781)\n",
            "('people', 782)\n",
            "('perceptible', 783)\n",
            "('perfect', 784)\n",
            "('persistence', 785)\n",
            "('persuasively', 786)\n",
            "('phrase', 787)\n",
            "('picture', 788)\n",
            "('pictures', 789)\n",
            "('pines', 790)\n",
            "('pink', 791)\n",
            "('place', 792)\n",
            "('placed', 793)\n",
            "('plain', 794)\n",
            "('platitudes', 795)\n",
            "('pleased', 796)\n",
            "('pockets', 797)\n",
            "('point', 798)\n",
            "('poised', 799)\n",
            "('poor', 800)\n",
            "('portrait', 801)\n",
            "('posing', 802)\n",
            "('possessed', 803)\n",
            "('poverty', 804)\n",
            "('predicted', 805)\n",
            "('preliminary', 806)\n",
            "('presenting', 807)\n",
            "('prestidigitation', 808)\n",
            "('pretty', 809)\n",
            "('previous', 810)\n",
            "('price', 811)\n",
            "('pride', 812)\n",
            "('pride:', 813)\n",
            "('princely', 814)\n",
            "('prism', 815)\n",
            "('problem', 816)\n",
            "('proclaiming', 817)\n",
            "('prodigious', 818)\n",
            "('profusion', 819)\n",
            "('protest', 820)\n",
            "('prove', 821)\n",
            "('public', 822)\n",
            "('purblind', 823)\n",
            "('purely', 824)\n",
            "('pushed', 825)\n",
            "('put', 826)\n",
            "('qualities', 827)\n",
            "('quality', 828)\n",
            "('queerly', 829)\n",
            "('question', 830)\n",
            "('question:', 831)\n",
            "('quickly', 832)\n",
            "('quietly', 833)\n",
            "('quite', 834)\n",
            "('quote', 835)\n",
            "('rain', 836)\n",
            "('raised', 837)\n",
            "('random', 838)\n",
            "('rather', 839)\n",
            "('re', 840)\n",
            "('real', 841)\n",
            "('really', 842)\n",
            "('reared', 843)\n",
            "('reason', 844)\n",
            "('reassurance', 845)\n",
            "('recovering', 846)\n",
            "('recreated', 847)\n",
            "('reflected', 848)\n",
            "('reflection', 849)\n",
            "('regrets', 850)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Putting it now all together into a tokenizer class\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "    #The encode function turns text into token IDs\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "        #The decode function turns token IDs back into text\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "VTRLp4UZQUiJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgC_Y9jEQpgi",
        "outputId": "73ff80a0-fc17-498c-894a-6691c8761d99"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
        "#These integers can then be embedded (later) as input of/for the LLM\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt6KvD1_RlKj",
        "outputId": "d6886ede-57de-495d-c9b5-43ac5e5e1553"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can decode the integers back into text\n",
        "\n",
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QumHeujISEIy",
        "outputId": "b9c2c02e-bfa5-4087-c2b8-4f2b56875dd9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AcTgh78YSbAP",
        "outputId": "36aec4ec-887b-4c70-90a0-56b5ca15ca56"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 does not use an <UNK> token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units.\n"
      ],
      "metadata": {
        "id": "LvVirgmiUNSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"sunlit\"\n",
        "\n",
        "tokenizer.encode(text)\n",
        "for i in range(980, 983):\n",
        "    print(tokenizer.decode([i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksVYCtyGUOOT",
        "outputId": "ccfd9ba5-7b48-4838-b030-2fae49dc5ba2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sunburnt\n",
            "sunlit\n",
            "superb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
        "(2)To deal with such cases, we can add special tokens like \"<|unk|>\" to the vocabulary to represent unknown words\n",
        "(3)let's add another token called \"<|endoftext|>\" which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
      ],
      "metadata": {
        "id": "_HkqNe3sUsOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "JN2GhK-4Ugh9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwQVWLRzV7ev",
        "outputId": "e10f7abc-43f3-48bb-87d7-86e266672c42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkAvLhnLV_uc",
        "outputId": "0198c729-3853-4f57-d25d-28c72b92ab94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1156)\n",
            "('your', 1157)\n",
            "('yourself', 1158)\n",
            "('<|endoftext|>', 1159)\n",
            "('<|unk|>', 1160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to adjust the tokenizer accordingly so that it knows when and how to use the new <unk> token"
      ],
      "metadata": {
        "id": "OfBRlC_CWSa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                        else \"<|unk|>\" for item in preprocessed]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "5TNsyyyAWRHc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's try to tokenize text with the modified tokenizer:"
      ],
      "metadata": {
        "id": "B0kdIi7LWeoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a13p_592WIsO",
        "outputId": "9a3fbd66-976f-4476-8304-7e158c8ddae5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6m4cfeRWvid",
        "outputId": "6caf3efb-67ee-4cba-8ce0-a14743e62bb3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1160,\n",
              " 5,\n",
              " 362,\n",
              " 1155,\n",
              " 642,\n",
              " 1000,\n",
              " 10,\n",
              " 1159,\n",
              " 57,\n",
              " 1013,\n",
              " 981,\n",
              " 1009,\n",
              " 738,\n",
              " 1013,\n",
              " 1160,\n",
              " 7]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cPQpVpwfW0oj",
        "outputId": "20c22a5f-2467-4170-a635-abbe44fd10e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pip install tiktoken\n",
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1thFu-6GYwKV",
        "outputId": "b735c870-1542-475b-9c44-7c5808c9e12c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "yidqMkuKZIg_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJPapLzGZNKn",
        "outputId": "de3a2c8e-e8d8-436b-ecf4-14e39fc6a593"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHvnhg20ZVOd",
        "outputId": "f417b9d6-93f5-4428-bff5-33e70ea779e6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/LLMs-from-scratch/ch02/01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfdR3Z7ZxZ1",
        "outputId": "10293784-b240-45a4-b42c-5d4eb0ced628"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each text chunk, we want the inputs and targets\n",
        "Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
      ],
      "metadata": {
        "id": "JIESFepsaCjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "8zKlmg3zZ81X"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voVyg0yZaKpv",
        "outputId": "da707104-18b8-4634-9fc1-154c4a60522c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One by one, the prediction would look like as follows:"
      ],
      "metadata": {
        "id": "FsdSuydsackw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74wDFKpHafTm",
        "outputId": "95b50af2-7061-4725-e6a5-8ceadfe36a09"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXVrrngGaogf",
        "outputId": "dfcb5a6b-7616-4bab-c45e-921cab56b15b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2n1-8rya7Ny",
        "outputId": "c68f60a5-3e83-43a3-abf3-df2bb777c4f1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset and dataloader that extract chunks from the input text dataset"
      ],
      "metadata": {
        "id": "78HVATCYbXW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "CamB7z91bAMg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "xRac84fgbYTY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "a8JMm15VbrGQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:"
      ],
      "metadata": {
        "id": "2KbsIMXsb0oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ajUqeZob1XH",
        "outputId": "64e7a248-53c8-4a55-d8fb-0b43f111f3be"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UqYLSp5cfQT",
        "outputId": "b0fadaed-a229-497e-85b6-b26befac2bf1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
      ],
      "metadata": {
        "id": "-_DSZ7Ngc3lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p2JMI0scgpA",
        "outputId": "52a7d960-8dfa-4718-e3c0-eb01df867824"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is already almost ready for an LLM\n",
        "But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
        "Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
      ],
      "metadata": {
        "id": "eJpjlo3DdYrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppose we have the following four input examples with input ids 5, 1, 3, and 2 (after tokenization):\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "OEoWLHJSdCYJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
      ],
      "metadata": {
        "id": "Q8LJASRSdo1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This would result in a 6x3 weight matrix:\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "oNIihkUzdm3Y"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQT3gbDvdgYI",
        "outputId": "3e3fc910-5e1b-4e17-9126-719ca4b02eb2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf1AaH5qd9Ew",
        "outputId": "4fe177e3-fa7e-4bd5-9c45-cd5c777a2bd6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYq08vm4flNU",
        "outputId": "a7fb18bf-4d3d-4bb1-dd79-60b7f620d2a9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BytePair encoder has a vocabulary size of 50,257:\n",
        "Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
      ],
      "metadata": {
        "id": "9btvk3sTh_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "TNbEwWYihCB0"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
        "If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:"
      ],
      "metadata": {
        "id": "hIl4VIXuidWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "EDUNgWe4ifzy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw6Z4tHjikO6",
        "outputId": "9e46fc69-7271-4365-fe3f-0662bbe23af4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luX-dtpdk1Rn",
        "outputId": "944fcfe8-51b2-4159-ecd3-5fc8bbce5262"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
      ],
      "metadata": {
        "id": "Chb7ArJyk65d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "ceM4vp4pk86T"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Wh72iClKKe",
        "outputId": "48065e06-4678-4eec-8b95-33c376523c5b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
      ],
      "metadata": {
        "id": "q8FgJJvClb0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNJxHUUXlM1P",
        "outputId": "c6ecb2c9-8d69-4ab4-f3c8-fb9b44aaa793"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}