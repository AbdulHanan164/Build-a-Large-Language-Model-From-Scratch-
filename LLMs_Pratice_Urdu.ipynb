{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_XWQDVZ3KP76"
      },
      "outputs": [],
      "source": [
        "#Load raw text we want to work with\n",
        "#The Verdict by Edith Wharton is a public domain short story\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXzdu9NLz6lx",
        "outputId": "3884efcd-8793-40b9-e654-ba1b4e376be0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A28bLDu9MvME",
        "outputId": "a29559fa-6dad-4588-c304-3894884ba971"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 1393\n",
            "میرے چلتے پیچھے پیچھے ایک دن، ایک چھوٹے سے گاؤں میں، ایک لڑکا نے اپنے دوستوں کے ساتھ ایک ماحولیاتی \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The following regular expression will split on whitespaces\n",
        "import re\n",
        "text = \"میرے چلتے پیچھے پیچھے ایک دن، ایک چھوٹے سے گاؤں میں، ایک لڑکا نے اپنے دوستوں کے ساتھ ایک ماحولیاتی\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD3bp86gN_4b",
        "outputId": "dfdce60a-c160-458a-d7e3-6fa43b665979"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['میرے', ' ', 'چلتے', ' ', 'پیچھے', ' ', 'پیچھے', ' ', 'ایک', ' ', 'دن،', ' ', 'ایک', ' ', 'چھوٹے', ' ', 'سے', ' ', 'گاؤں', ' ', 'میں،', ' ', 'ایک', ' ', 'لڑکا', ' ', 'نے', ' ', 'اپنے', ' ', 'دوستوں', ' ', 'کے', ' ', 'ساتھ', ' ', 'ایک', ' ', 'ماحولیاتی']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well\n",
        "\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqejpCSbOhPK",
        "outputId": "ed6f0f00-af41-4638-f8a9-1edbc908241a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['میرے', ' ', 'چلتے', ' ', 'پیچھے', ' ', 'پیچھے', ' ', 'ایک', ' ', 'دن،', ' ', 'ایک', ' ', 'چھوٹے', ' ', 'سے', ' ', 'گاؤں', ' ', 'میں،', ' ', 'ایک', ' ', 'لڑکا', ' ', 'نے', ' ', 'اپنے', ' ', 'دوستوں', ' ', 'کے', ' ', 'ساتھ', ' ', 'ایک', ' ', 'ماحولیاتی']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's also handle other types of punctuation, such as periods, question marks, and so on\n",
        "text = \"ہیلو!، دنیا۔ ہاں، یہ ایک ٹیسٹ ہے?۔\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NExM7RziOzQd",
        "outputId": "ee9540dc-ef26-47c2-9016-684516e938f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ہیلو', '!', '،', 'دنیا۔', 'ہاں،', 'یہ', 'ایک', 'ٹیسٹ', 'ہے', '?', '۔']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKk4N5iRPFcD",
        "outputId": "fe92e213-90ea-4588-f619-21b3e14b8921"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['میرے', 'چلتے', 'پیچھے', 'پیچھے', 'ایک', 'دن،', 'ایک', 'چھوٹے', 'سے', 'گاؤں', 'میں،', 'ایک', 'لڑکا', 'نے', 'اپنے', 'دوستوں', 'کے', 'ساتھ', 'ایک', 'ماحولیاتی', 'سیاحت', 'کی', 'خواب', 'میں', 'بھرپور', 'جویا۔', 'گاؤں', 'کے', 'باہر', 'جنگلاتی']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's calculate the total number of tokens\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPo9PF5NPNJ6",
        "outputId": "ad88ae94-17a0-4069-b0e1-b81f8bb23f16"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#From these tokens, we can now build a vocabulary that consists of all the unique tokens\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyPGFgpPPRjK",
        "outputId": "5ff675b1-ec08-4217-cb2a-a740e2e99b14"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "dwg5A8IZPkkK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Below are the first 50 entries in this vocabulary:\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 140:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_JJ5DpbPr2y",
        "outputId": "e79b9a08-aecd-4342-d0dc-bcb30a52b5b9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('آخری', 0)\n",
            "('اب', 1)\n",
            "('احترام', 2)\n",
            "('احساس', 3)\n",
            "('احساسات', 4)\n",
            "('اس', 5)\n",
            "('ان', 6)\n",
            "('انہوں', 7)\n",
            "('اور', 8)\n",
            "('اُن', 9)\n",
            "('اُنہوں', 10)\n",
            "('اُٹھایا۔', 11)\n",
            "('اپنے', 12)\n",
            "('ایک', 13)\n",
            "('بابون', 14)\n",
            "('باہر', 15)\n",
            "('بعد،', 16)\n",
            "('بنائی', 17)\n",
            "('بڑھ', 18)\n",
            "('بڑھتی', 19)\n",
            "('بھرپور', 20)\n",
            "('بہتر', 21)\n",
            "('تو', 22)\n",
            "('تک', 23)\n",
            "('تھا', 24)\n",
            "('تھا،', 25)\n",
            "('تھا۔', 26)\n",
            "('تھی', 27)\n",
            "('تھی۔', 28)\n",
            "('جا', 29)\n",
            "('جاری', 30)\n",
            "('جاننے', 31)\n",
            "('جانوروں', 32)\n",
            "('جب', 33)\n",
            "('جذبہ', 34)\n",
            "('جنگل', 35)\n",
            "('جنگلاتی', 36)\n",
            "('جو', 37)\n",
            "('جویا۔', 38)\n",
            "('جگہوں', 39)\n",
            "('حریم', 40)\n",
            "('خواب', 41)\n",
            "('خواہش', 42)\n",
            "('خوبصورت', 43)\n",
            "('خوبصورتی', 44)\n",
            "('خوشی', 45)\n",
            "('دعویٰ', 46)\n",
            "('دل', 47)\n",
            "('دن،', 48)\n",
            "('دوستوں', 49)\n",
            "('دیا،', 50)\n",
            "('دینے', 51)\n",
            "('دیکھا', 52)\n",
            "('دیکھتے', 53)\n",
            "('دیکھیں', 54)\n",
            "('راستہ', 55)\n",
            "('رخ', 56)\n",
            "('رکھتے', 57)\n",
            "('رہے۔', 58)\n",
            "('زمانے', 59)\n",
            "('زندگی', 60)\n",
            "('ساتھ', 61)\n",
            "('سفر', 62)\n",
            "('سمجھایا', 63)\n",
            "('سکتے،', 64)\n",
            "('سیاحت', 65)\n",
            "('سے', 66)\n",
            "('شروع', 67)\n",
            "('شریک', 68)\n",
            "('طرف', 69)\n",
            "('طریقے', 70)\n",
            "('عجیب', 71)\n",
            "('علاقے', 72)\n",
            "('فیصلہ', 73)\n",
            "('قائم', 74)\n",
            "('قدیم', 75)\n",
            "('قریب', 76)\n",
            "('قسم', 77)\n",
            "('لطف', 78)\n",
            "('لوٹے', 79)\n",
            "('لوگوں', 80)\n",
            "('لڑکا', 81)\n",
            "('لڑکے', 82)\n",
            "('لگے', 83)\n",
            "('لیکن', 84)\n",
            "('ماحولیاتی', 85)\n",
            "('مانگی', 86)\n",
            "('متشددی', 87)\n",
            "('محبت', 88)\n",
            "('محترمی', 89)\n",
            "('محسوس', 90)\n",
            "('مختلف', 91)\n",
            "('مسجد', 92)\n",
            "('معاشرتی', 93)\n",
            "('معافی', 94)\n",
            "('مغرور', 95)\n",
            "('مقام', 96)\n",
            "('ملاقاتوں', 97)\n",
            "('منظر', 98)\n",
            "('میرے', 99)\n",
            "('میں', 100)\n",
            "('میں،', 101)\n",
            "('نہیں', 102)\n",
            "('نیا', 103)\n",
            "('نے', 104)\n",
            "('وہ', 105)\n",
            "('وہاں', 106)\n",
            "('پرانی', 107)\n",
            "('پودوں', 108)\n",
            "('پچھاڑا', 109)\n",
            "('پہنچ', 110)\n",
            "('پیدا', 111)\n",
            "('پیچھے', 112)\n",
            "('چاروں', 113)\n",
            "('چاہتا', 114)\n",
            "('چاہتے', 115)\n",
            "('چلتے', 116)\n",
            "('چھوٹے', 117)\n",
            "('کا', 118)\n",
            "('کبھی', 119)\n",
            "('کر،', 120)\n",
            "('کرنے', 121)\n",
            "('کو', 122)\n",
            "('کھیلنے', 123)\n",
            "('کہ', 124)\n",
            "('کی', 125)\n",
            "('کیا،', 126)\n",
            "('کیا۔', 127)\n",
            "('کے', 128)\n",
            "('گئی', 129)\n",
            "('گئی۔', 130)\n",
            "('گاؤں', 131)\n",
            "('گاوں', 132)\n",
            "('ہوئے', 133)\n",
            "('ہوئے،', 134)\n",
            "('ہوا', 135)\n",
            "('ہوا۔', 136)\n",
            "('ہوتے', 137)\n",
            "('ہیں۔', 138)\n",
            "('ہے۔', 139)\n",
            "('<|endoftext|>', 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.findall(r'\\S+|\\n', text)\n",
        "        ids = [self.str_to_int[s] for s in preprocessed if s in self.str_to_int]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VTRLp4UZQUiJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"اور وہ اب اپنے گاوں کو اور اس کے لوگوں کو اور بہتر طریقے سے جاننے کی خواہش رکھتے ہیں\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgC_Y9jEQpgi",
        "outputId": "67f71fbf-7fef-48e8-b4a3-4846b9d6eda0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 105, 1, 12, 132, 122, 8, 5, 128, 80, 122, 8, 21, 70, 66, 31, 125, 42, 57]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
        "#These integers can then be embedded (later) as input of/for the LLM\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"اور وہ اب اپنے گاوں کو اور اس کے لوگوں کو اور بہتر طریقے سے جاننے کی خواہش رکھتے ہیں\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt6KvD1_RlKj",
        "outputId": "529572f4-838e-479d-990e-b114e02c1cb9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[105, 1, 12, 132, 122, 8, 5, 128, 80, 122, 8, 21, 70, 66, 31, 125, 42, 57]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can decode the integers back into text\n",
        "\n",
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QumHeujISEIy",
        "outputId": "0a88ab49-010a-43e0-9598-bb33cb6fa80a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وہ اب اپنے گاوں کو اور اس کے لوگوں کو اور بہتر طریقے سے جاننے کی خواہش رکھتے'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AcTgh78YSbAP",
        "outputId": "ccbb32e0-a4e5-4876-ba01-1dede97e3ca3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وہ اب اپنے گاوں کو اور اس کے لوگوں کو اور بہتر طریقے سے جاننے کی خواہش رکھتے'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 does not use an <UNK> token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units.\n"
      ],
      "metadata": {
        "id": "LvVirgmiUNSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"کھیلنے\"\n",
        "\n",
        "tokenizer.encode(text)\n",
        "for i in range(120, 130):\n",
        "    print(tokenizer.decode([i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksVYCtyGUOOT",
        "outputId": "0cb5553e-5001-4cd4-898a-c87bae92c6d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کر،\n",
            "کرنے\n",
            "کو\n",
            "کھیلنے\n",
            "کہ\n",
            "کی\n",
            "کیا،\n",
            "کیا۔\n",
            "کے\n",
            "گئی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
        "(2)To deal with such cases, we can add special tokens like \"<|unk|>\" to the vocabulary to represent unknown words\n",
        "(3)let's add another token called \"<|endoftext|>\" which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
      ],
      "metadata": {
        "id": "_HkqNe3sUsOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "JN2GhK-4Ugh9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwQVWLRzV7ev",
        "outputId": "afa5086b-2030-4bc4-d1a8-9bad80fc4bd5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkAvLhnLV_uc",
        "outputId": "d08d6c08-eafa-49dd-e54f-8ddc10edc612"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ہوتے', 137)\n",
            "('ہیں۔', 138)\n",
            "('ہے۔', 139)\n",
            "('<|endoftext|>', 140)\n",
            "('<|unk|>', 141)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to adjust the tokenizer accordingly so that it knows when and how to use the new <unk> token"
      ],
      "metadata": {
        "id": "OfBRlC_CWSa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int\n",
        "                        else \"<|unk|>\" for item in preprocessed]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "5TNsyyyAWRHc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's try to tokenize text with the modified tokenizer:"
      ],
      "metadata": {
        "id": "B0kdIi7LWeoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"اُن کے دوستوں نے اُن کو سمجھایا کہ وہ جانوروں کی حریم میں متشددی سے نہیں جا سکتے، تو اُنہوں نے معافی مانگی اور گاؤں کی طرف رخ کیا۔؟\"\n",
        "text2 = \"محل کی روشنی دار تراسوں میں\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a13p_592WIsO",
        "outputId": "2b65fb1c-cbd3-405b-f62a-1684a40f3acf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اُن کے دوستوں نے اُن کو سمجھایا کہ وہ جانوروں کی حریم میں متشددی سے نہیں جا سکتے، تو اُنہوں نے معافی مانگی اور گاؤں کی طرف رخ کیا۔؟ <|endoftext|> محل کی روشنی دار تراسوں میں۔\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6m4cfeRWvid",
        "outputId": "56c9d704-4c65-465a-f133-48ef23d70e88"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9,\n",
              " 128,\n",
              " 49,\n",
              " 104,\n",
              " 9,\n",
              " 122,\n",
              " 63,\n",
              " 124,\n",
              " 105,\n",
              " 32,\n",
              " 125,\n",
              " 40,\n",
              " 100,\n",
              " 87,\n",
              " 66,\n",
              " 102,\n",
              " 29,\n",
              " 64,\n",
              " 22,\n",
              " 10,\n",
              " 104,\n",
              " 94,\n",
              " 86,\n",
              " 8,\n",
              " 131,\n",
              " 125,\n",
              " 69,\n",
              " 56,\n",
              " 141,\n",
              " 140,\n",
              " 141,\n",
              " 125,\n",
              " 141,\n",
              " 141,\n",
              " 141,\n",
              " 141]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cPQpVpwfW0oj",
        "outputId": "39e548ea-e50b-43b8-ae51-3b2b55150a70"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'اُن کے دوستوں نے اُن کو سمجھایا کہ وہ جانوروں کی حریم میں متشددی سے نہیں جا سکتے، تو اُنہوں نے معافی مانگی اور گاؤں کی طرف رخ <|unk|> <|endoftext|> <|unk|> کی <|unk|> <|unk|> <|unk|> <|unk|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pip install tiktoken\n",
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1thFu-6GYwKV",
        "outputId": "08dcd500-2894-4380-aad0-c49bb653e48c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "yidqMkuKZIg_"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"اُن کے دوستوں نے اُن کو سمجھایا کہ وہ جانوروں کی حریم میں متشددی سے نہیں جا سکتے، تو اُنہوں نے معافی مانگی اور گاؤں کی طرف رخ کیا۔ <|endoftext|>محل کی روشنی دار تراسوں میں.\"\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJPapLzGZNKn",
        "outputId": "04258bbb-7c52-462b-eabc-08e5e4baa74d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12919, 149, 237, 23338, 220, 150, 102, 151, 240, 17550, 107, 30335, 45692, 41486, 30335, 150, 118, 18923, 228, 151, 240, 220, 12919, 149, 237, 23338, 220, 150, 102, 30335, 17550, 111, 25405, 148, 105, 150, 122, 12919, 151, 234, 12919, 220, 150, 102, 151, 223, 42092, 151, 223, 17550, 105, 12919, 23338, 30335, 26897, 30335, 150, 118, 220, 150, 102, 151, 234, 17550, 255, 26897, 151, 234, 25405, 47048, 151, 234, 150, 118, 47048, 41486, 148, 112, 38843, 38843, 151, 234, 17550, 111, 151, 240, 18923, 228, 151, 223, 151, 234, 150, 118, 17550, 105, 12919, 17550, 111, 150, 102, 41486, 151, 240, 148, 234, 17550, 103, 30335, 220, 12919, 149, 237, 23338, 151, 223, 30335, 150, 118, 18923, 228, 151, 240, 47048, 44690, 12919, 149, 223, 151, 234, 47048, 12919, 23338, 150, 107, 151, 234, 220, 12919, 30335, 26897, 220, 150, 107, 34247, 97, 150, 118, 220, 150, 102, 151, 234, 17550, 115, 26897, 149, 223, 17550, 109, 148, 106, 220, 150, 102, 151, 234, 12919, 151, 242, 220, 50256, 25405, 148, 255, 13862, 220, 150, 102, 151, 234, 17550, 109, 30335, 148, 112, 23338, 151, 234, 17550, 107, 12919, 26897, 17550, 103, 26897, 34247, 111, 30335, 150, 118, 47048, 151, 234, 150, 118, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHvnhg20ZVOd",
        "outputId": "04623ef0-719b-40e9-e42d-9af6eef6638c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "اُن کے دوستوں نے اُن کو سمجھایا کہ وہ جانوروں کی حریم میں متشددی سے نہیں جا سکتے، تو اُنہوں نے معافی مانگی اور گاؤں کی طرف رخ کیا۔ <|endoftext|>محل کی روشنی دار تراسوں میں.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/Urdu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bfdR3Z7ZxZ1",
        "outputId": "495b7754-6742-4716-cbeb-fd2a2be66e88"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each text chunk, we want the inputs and targets\n",
        "Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
      ],
      "metadata": {
        "id": "JIESFepsaCjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "enc_sample = enc_text[130:]"
      ],
      "metadata": {
        "id": "8zKlmg3zZ81X"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voVyg0yZaKpv",
        "outputId": "0c1e008a-4ea9-46f4-b9da-5e854807e3f6"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [30335, 13862, 151, 234]\n",
            "y:      [13862, 151, 234, 34247]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One by one, the prediction would look like as follows:"
      ],
      "metadata": {
        "id": "FsdSuydsackw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74wDFKpHafTm",
        "outputId": "0e37d01f-deb4-4c55-9072-0095d4082a2e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30335] ----> 13862\n",
            "[30335, 13862] ----> 151\n",
            "[30335, 13862, 151] ----> 234\n",
            "[30335, 13862, 151, 234] ----> 34247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXVrrngGaogf",
        "outputId": "463f14ed-cdf7-4b11-c30a-1a90148b5fca"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "و ----> ل\n",
            "ول ----> �\n",
            "ول� ----> �\n",
            "ولی ----> ا�\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2n1-8rya7Ny",
        "outputId": "3c25eefd-58f5-40c7-a358-8cde90af7f73"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.3.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataset and dataloader that extract chunks from the input text dataset"
      ],
      "metadata": {
        "id": "78HVATCYbXW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "CamB7z91bAMg"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "xRac84fgbYTY"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "a8JMm15VbrGQ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:"
      ],
      "metadata": {
        "id": "2KbsIMXsb0oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ajUqeZob1XH",
        "outputId": "499a36b7-6ba4-4a04-9b27-13e90aa650c2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[25405,   151,   234, 26897]]), tensor([[  151,   234, 26897,   151]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UqYLSp5cfQT",
        "outputId": "dbc5956c-9ff1-4fab-9819-730271daf366"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  151,   234, 26897,   151]]), tensor([[  234, 26897,   151,   240]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
      ],
      "metadata": {
        "id": "-_DSZ7Ngc3lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p2JMI0scgpA",
        "outputId": "2aad25de-d4df-492a-813b-7cd51b123df2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[25405,   151,   234, 26897],\n",
            "        [  151,   240,   220,   150],\n",
            "        [  228, 13862, 41486,   151],\n",
            "        [  240, 18923,   122,   151],\n",
            "        [  234,   150,   228,   150],\n",
            "        [  122,   151,   240, 18923],\n",
            "        [  122,   151,   234,   150],\n",
            "        [  228,   150,   122,   151]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  151,   234, 26897,   151],\n",
            "        [  240,   220,   150,   228],\n",
            "        [13862, 41486,   151,   240],\n",
            "        [18923,   122,   151,   234],\n",
            "        [  150,   228,   150,   122],\n",
            "        [  151,   240, 18923,   122],\n",
            "        [  151,   234,   150,   228],\n",
            "        [  150,   122,   151,   240]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is already almost ready for an LLM\n",
        "But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
        "Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
      ],
      "metadata": {
        "id": "eJpjlo3DdYrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppose we have the following four input examples with input ids 5, 1, 3, and 2 (after tokenization):\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "OEoWLHJSdCYJ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
      ],
      "metadata": {
        "id": "Q8LJASRSdo1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This would result in a 6x3 weight matrix:\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "oNIihkUzdm3Y"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQT3gbDvdgYI",
        "outputId": "dec3a94d-b3a1-44c4-f00b-9b6cd0a9906c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf1AaH5qd9Ew",
        "outputId": "f07261fb-76c1-4c17-c99e-3fc3a4242bf8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYq08vm4flNU",
        "outputId": "dc3be4ad-cf07-4139-f684-c65321f0159e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BytePair encoder has a vocabulary size of 50,257:\n",
        "Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
      ],
      "metadata": {
        "id": "9btvk3sTh_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "TNbEwWYihCB0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
        "If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:"
      ],
      "metadata": {
        "id": "hIl4VIXuidWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "EDUNgWe4ifzy"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw6Z4tHjikO6",
        "outputId": "db0e6317-9075-4e4b-a5cf-db09467d7c1b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[25405,   151,   234, 26897],\n",
            "        [  151,   240,   220,   150],\n",
            "        [  228, 13862, 41486,   151],\n",
            "        [  240, 18923,   122,   151],\n",
            "        [  234,   150,   228,   150],\n",
            "        [  122,   151,   240, 18923],\n",
            "        [  122,   151,   234,   150],\n",
            "        [  228,   150,   122,   151]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luX-dtpdk1Rn",
        "outputId": "6d570e77-5d94-4914-9d37-5b05031e29e4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
      ],
      "metadata": {
        "id": "Chb7ArJyk65d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "ceM4vp4pk86T"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4Wh72iClKKe",
        "outputId": "197fb34c-f76b-4556-bb77-c425eea741f5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
      ],
      "metadata": {
        "id": "q8FgJJvClb0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNJxHUUXlM1P",
        "outputId": "8bb1d996-ce1d-451b-a65d-bd31d44d7c59"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}